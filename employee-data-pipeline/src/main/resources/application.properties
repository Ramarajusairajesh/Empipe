spring.application.name=employee-data-pipeline

# Server Configuration
server.port=8080

# Database Configuration
spring.datasource.url=jdbc:postgresql://postgres:5432/employee_pipeline
spring.datasource.username=postgres
spring.datasource.password=postgres
spring.datasource.driver-class-name=org.postgresql.Driver

# JPA (Hibernate) Configuration
# 'update' will create/update schema based on entities. Use 'none' or 'validate' in production.
spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect

# Kafka Configuration
spring.kafka.bootstrap-servers=kafka:29092
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer
spring.kafka.producer.properties.spring.json.add.type.headers=false # Avoid adding type headers if not strictly needed

# Consumer Configuration (for receiving messages from Kafka)
spring.kafka.consumer.group-id=employee-pipeline-group
spring.kafka.consumer.auto-offset-reset=earliest # Read from the beginning if no offset is found
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.consumer.properties.spring.json.trusted.packages=com.empipeline.model
spring.kafka.consumer.properties.spring.json.use.type.headers=false
spring.kafka.consumer.properties.spring.json.value.default.type=com.empipeline.model.EmployeeEvent

# Kafka Topics
kafka.topic.employee=employee-events

# Logging Configuration
logging.level.root=INFO
logging.level.com.empipeline=DEBUG
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n

# --- Application Specific Properties ---
# Define Kafka topics
app.kafka.topics.employee-events=employee-events-topic

